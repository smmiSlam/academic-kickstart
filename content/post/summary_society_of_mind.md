+++
title = "Summary on The Society of Mind"

date = 2021-04-02T00:00:00
lastmod = 2020-04-02T00:05:05
draft = false

# Authors. Comma separated list, e.g. `["Bob Smith", "David Jones"]`.
authors = ["SM Mazharul Islam"]

tags = ["Summary", "CSE6369", "Model"]
summary = "This is my summary of the book **The Society of Mind** by **Marvin Minsky**"

[header]
image = ""
caption = ""

+++

# Introduction

In this post I have tried to summarize the key points of the the book **The Society of Mind** by **Marvin Minsky**. Here the author have interesting propositions about how our mind wroks and how can go forward to design a model of our mind. People who are looking for fresh perspective towards HL-AI would be an ideal audience.

This is part of a paper review assignment of the course **CSE6369**.

Thank you for taking the time to read this article! :heart:


# My Summary

Our mind is actually a collection of myriads of mini-minds connected in a **hierarchical** manner and each of such mini-minds can have similar structures. As a result if we could sample one of these mini-minds we would be able to see that it performs a very basic set of managing tasks that seems quite foreign to what the mind is actually trying to attain. Each such mini-minds are also equipped with limited **short-term memories** which allow them to keep track of near past. In this book the author proposes each such mini-mind to have a duel life. As an agency it knows what it is doing but as an agent it cannot know anything at all.

Whenever the agents under a agency has some quarrel in between them, they can not come to an agreement, this weaken the agencies power to supress the other peer agencies. Once an agency loses its dominance it keeps on functioning its job, just it does not weigh much toward the upward flow. of information. At a later chance, this agency can seige an opportunity to gain back control. This also probably explains how we come to some solution/answer of a problem/question almost instantly.

Our **long-term memory** and the sensory inputs are always playing a balance game. Imagine thinking about a square circle. The noion is bizarre and alien to us since our long term-memory has no recollection of such an object. Here, the long term memory can override the sensory input. Now imaine looking at a red ball and then trying to see a white ball instead. Even though our long-term memory has seem both these objects, our sensory input here disallowes us to invoke any such illusion or hallucination (except for the use of hallucinogenic drugs may be).

**Common-sense** is the culmination of numerous layers of learnt skills from our early years of development. As we age, more and more layers of complex learning skills develop on top of the simpler ones. As a result we have this amnesia about the completion process of very basic level tasks. The author hence suggests to look for inspiration on **simpler forms** of life (a worm or a virus or may be a human baby) so that we can understand these early phases to find useful clues.


Since these short term memroies have limited capacity, thinking about how a particular though came to existance is quite a tricky business. Cause we ask such a question, the related short-term memories are now trying to answer this questiona and as a result the states of these units are over-written with these later question erasing most of the states that invoked the thought in the first place. The author suggests carefully designed **mental experiments** to break this endless loop.



# Ending Thoughts
I think the idea about a hierarchical connectivity of our mind (or brain) is a powerful one. Utilizing that we can parhaps implement a model that can perform longitudinal learning and curriculum learing.

The author here draws the concept of agencies that can supervise an agency that is observing and acting on the real world. This is quite surprising and eye-opening to me that a book from 1986 had the idea of a actor-critic based RL models which is used nowadays extensively. These type of dual system can also be found in Auto-encoder, Generative Adversarial Network.

However, the author's comment about seemingly distant agencies to be oblivious about each others actions may not be entirely true. As the information flows up the hierarchy for various simpler agencies, a higher level agent can make sense of these information and issue an command down the rank. So, it may be the case that this particular higher level agent had access to information from various distant agent the single higher level instruction can at the end reach them all (more or less). So, it seems to me that these agents are at the very least all indirectly connected.


# Further Reading List
- [A Thousand Brain](https://www.amazon.com/Thousand-Brains-New-Theory-Intelligence/dp/1541675819)
- [Longitudinal Development](https://en.wikipedia.org/wiki/Longitudinal_study)
- [Open-ended Learning](https://thehomeschoolscientist.com/open-ended-learning-resource/)
